{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Initial testing and installs\n\n---","metadata":{}},{"cell_type":"code","source":"#Testing\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-22T11:16:39.688328Z","iopub.execute_input":"2025-12-22T11:16:39.688570Z","iopub.status.idle":"2025-12-22T11:16:41.004134Z","shell.execute_reply.started":"2025-12-22T11:16:39.688544Z","shell.execute_reply":"2025-12-22T11:16:41.003184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q google-generativeai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T11:19:14.040109Z","iopub.execute_input":"2025-12-22T11:19:14.040939Z","iopub.status.idle":"2025-12-22T11:19:18.665288Z","shell.execute_reply.started":"2025-12-22T11:19:14.040901Z","shell.execute_reply":"2025-12-22T11:19:18.664208Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import google.generativeai as genai\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T11:19:27.230495Z","iopub.execute_input":"2025-12-22T11:19:27.230827Z","iopub.status.idle":"2025-12-22T11:19:29.650111Z","shell.execute_reply.started":"2025-12-22T11:19:27.230796Z","shell.execute_reply":"2025-12-22T11:19:29.649273Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Gen AI Project - Smart Document Analyzer\n\n**Author:** Shamir Tanna \n**Date:** December 22, 2025  \n\n---\n\n## Project Overview\n\nThis project demonstrates 3 Generative AI capabilities using Google's Gemini API:\n\n1. **ðŸ“ Text Summarization** - Condensing long documents into key insights\n2. **ðŸ” Semantic Search with Embeddings** - Finding relevant sentences based on meaning\n3. **ðŸ“Š Code Execution** - Statistical analysis of text\n\n---\n\n## Use Case\n\nA complete document analysis system that can summarize content, find key sentences, and provide statistical insights automatically.\n\n---","metadata":{}},{"cell_type":"markdown","source":"# Capability 1: Summarization","metadata":{}},{"cell_type":"code","source":"import google.generativeai as genai\n\n# Configure (you already have this)\ngenai.configure(api_key='AIzaSyAdyo5cEPVjzysw2IsRIcVTEtCDhICyHio')\n\n# Create basic model for summarization\nmodel = genai.GenerativeModel('models/gemini-2.5-flash')\n\nprint(\"âœ… Model ready for summarization!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T11:22:25.336638Z","iopub.execute_input":"2025-12-22T11:22:25.337787Z","iopub.status.idle":"2025-12-22T11:22:25.342891Z","shell.execute_reply.started":"2025-12-22T11:22:25.337745Z","shell.execute_reply":"2025-12-22T11:22:25.342067Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Smart Document Analyzer - Gen AI Capstone","metadata":{}},{"cell_type":"code","source":"# Sample article to test with\nsample_article = \"\"\"\nArtificial intelligence has transformed dramatically in recent years. \nMachine learning models have become increasingly sophisticated, enabling \napplications ranging from natural language processing to computer vision. \nCompanies across industries are adopting AI to improve efficiency and \ncreate new products. However, concerns about AI safety, bias, and \nethical implications continue to be debated. Researchers are working \non making AI systems more transparent and accountable. The future of \nAI development will likely focus on creating systems that are both \npowerful and aligned with human values. Education and workforce \ntraining are also adapting to prepare people for an AI-driven economy.\n\"\"\"\n\n# Test basic generation first\nresponse = model.generate_content(\"Say hello!\")\nprint(response.text)\nprint(\"\\nâœ… Basic generation works!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T11:22:34.785084Z","iopub.execute_input":"2025-12-22T11:22:34.786082Z","iopub.status.idle":"2025-12-22T11:22:35.725904Z","shell.execute_reply.started":"2025-12-22T11:22:34.786046Z","shell.execute_reply":"2025-12-22T11:22:35.725109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def summarize_text(text, model):\n    \"\"\"\n    Capability 1: Summarization\n    Takes long text and creates a concise summary\n    \"\"\"\n    prompt = f\"\"\"\n    Summarize the following text in 2-3 sentences. \n    Focus on the main points.\n    \n    Text: {text}\n    \n    Summary:\n    \"\"\"\n    \n    response = model.generate_content(prompt)\n    return response.text\n\n# Test it!\nsummary = summarize_text(sample_article, model)\nprint(\"=== SUMMARIZATION (Capability 1) ===\")\nprint(summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T11:22:46.066702Z","iopub.execute_input":"2025-12-22T11:22:46.067055Z","iopub.status.idle":"2025-12-22T11:22:50.320190Z","shell.execute_reply.started":"2025-12-22T11:22:46.067027Z","shell.execute_reply":"2025-12-22T11:22:50.319400Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def summarize_text_advanced(text, model, summary_type=\"brief\"):\n    \"\"\"\n    Enhanced summarization with different styles\n    \"\"\"\n    prompts = {\n        \"brief\": f\"Summarize this in 1-2 sentences:\\n\\n{text}\",\n        \"detailed\": f\"Provide a detailed summary with key points:\\n\\n{text}\",\n        \"bullet_points\": f\"Summarize this as bullet points:\\n\\n{text}\"\n    }\n    \n    response = model.generate_content(prompts[summary_type])\n    return response.text\n\n# Test all three types\nprint(\"=== BRIEF SUMMARY ===\")\nprint(summarize_text_advanced(sample_article, model, \"brief\"))\n\nprint(\"\\n=== DETAILED SUMMARY ===\")\nprint(summarize_text_advanced(sample_article, model, \"detailed\"))\n\nprint(\"\\n=== BULLET POINTS ===\")\nprint(summarize_text_advanced(sample_article, model, \"bullet_points\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T11:23:18.337938Z","iopub.execute_input":"2025-12-22T11:23:18.338290Z","iopub.status.idle":"2025-12-22T11:23:32.637783Z","shell.execute_reply.started":"2025-12-22T11:23:18.338260Z","shell.execute_reply":"2025-12-22T11:23:32.636986Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Capability 2: Embeddings & Similarity Analysis\n## Finding related content using semantic embeddings","metadata":{}},{"cell_type":"code","source":"import google.generativeai as genai\n\ngenai.configure(api_key='AIzaSyAdyo5cEPVjzysw2IsRIcVTEtCDhICyHio')\n\n# Get embedding for a single piece of text\nresult = genai.embed_content(\n    model=\"models/text-embedding-004\",\n    content=\"I love pizza\"\n)\n\n# This is the embedding - a list of numbers!\nembedding = result['embedding']\n\nprint(f\"âœ… Embedding created!\")\nprint(f\"Length: {len(embedding)} numbers\")\nprint(f\"First 5 numbers: {embedding[:5]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T11:24:08.412441Z","iopub.execute_input":"2025-12-22T11:24:08.413251Z","iopub.status.idle":"2025-12-22T11:24:08.763330Z","shell.execute_reply.started":"2025-12-22T11:24:08.413219Z","shell.execute_reply":"2025-12-22T11:24:08.762525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sample texts to compare\ntexts = [\n    \"I love eating pizza and pasta\",\n    \"Italian food is my favorite cuisine\",\n    \"I enjoy playing basketball and soccer\",\n    \"Dogs are the best pets ever\",\n    \"Cats make wonderful companions\"\n]\n\n# Get embeddings for all texts\nembeddings = []\nfor text in texts:\n    result = genai.embed_content(\n        model=\"models/text-embedding-004\",\n        content=text\n    )\n    embeddings.append(result['embedding'])\n\nprint(f\"âœ… Created {len(embeddings)} embeddings\")\nprint(f\"Each embedding has {len(embeddings[0])} dimensions\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T11:24:28.444677Z","iopub.execute_input":"2025-12-22T11:24:28.445043Z","iopub.status.idle":"2025-12-22T11:24:29.427989Z","shell.execute_reply.started":"2025-12-22T11:24:28.445015Z","shell.execute_reply":"2025-12-22T11:24:29.427214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef cosine_similarity(vec1, vec2):\n    \"\"\"\n    Calculate how similar two embeddings are\n    Returns a score between -1 and 1 (higher = more similar)\n    \"\"\"\n    vec1 = np.array(vec1)\n    vec2 = np.array(vec2)\n    \n    dot_product = np.dot(vec1, vec2)\n    magnitude1 = np.linalg.norm(vec1)\n    magnitude2 = np.linalg.norm(vec2)\n    \n    return dot_product / (magnitude1 * magnitude2)\n\n# Test it!\nsimilarity = cosine_similarity(embeddings[0], embeddings[1])\nprint(f\"Similarity between text 0 and text 1: {similarity:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T11:24:37.994580Z","iopub.execute_input":"2025-12-22T11:24:37.995165Z","iopub.status.idle":"2025-12-22T11:24:38.001219Z","shell.execute_reply.started":"2025-12-22T11:24:37.995136Z","shell.execute_reply":"2025-12-22T11:24:38.000390Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_most_similar(query_text, corpus_texts, model_name=\"models/text-embedding-004\"):\n    \"\"\"\n    Capability 2: Similarity Search\n    Find which texts are most similar to a query\n    \"\"\"\n    \n    # Get embedding for query\n    query_result = genai.embed_content(\n        model=model_name,\n        content=query_text\n    )\n    query_embedding = query_result['embedding']\n    \n    # Get embeddings for all corpus texts\n    corpus_embeddings = []\n    for text in corpus_texts:\n        result = genai.embed_content(\n            model=model_name,\n            content=text\n        )\n        corpus_embeddings.append(result['embedding'])\n    \n    # Calculate similarities\n    similarities = []\n    for i, corpus_embedding in enumerate(corpus_embeddings):\n        sim = cosine_similarity(query_embedding, corpus_embedding)\n        similarities.append({\n            'text': corpus_texts[i],\n            'similarity': sim\n        })\n    \n    # Sort by similarity (highest first)\n    similarities.sort(key=lambda x: x['similarity'], reverse=True)\n    \n    return similarities\n\n# Test it!\nquery = \"I like Italian restaurants\"\nresults = find_most_similar(query, texts)\n\nprint(f\"=== SIMILARITY SEARCH (Capability 2) ===\")\nprint(f\"Query: '{query}'\\n\")\nprint(\"Most similar texts:\")\nfor i, result in enumerate(results, 1):\n    print(f\"{i}. [{result['similarity']:.3f}] {result['text']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T11:24:48.736593Z","iopub.execute_input":"2025-12-22T11:24:48.737228Z","iopub.status.idle":"2025-12-22T11:24:49.932866Z","shell.execute_reply.started":"2025-12-22T11:24:48.737199Z","shell.execute_reply":"2025-12-22T11:24:49.931829Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Capability 3: Code Execution & Analysis\n## Using Gemini to write and run Python code for text analysis","metadata":{}},{"cell_type":"code","source":"import google.generativeai as genai\n\ngenai.configure(api_key='AIzaSyAdyo5cEPVjzysw2IsRIcVTEtCDhICyHio')\n\ncode_model = genai.GenerativeModel(\n    'models/gemini-2.5-flash',\n    tools='code_execution'\n)\nprint(\"Code execution model ready!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T11:26:00.099249Z","iopub.execute_input":"2025-12-22T11:26:00.099599Z","iopub.status.idle":"2025-12-22T11:26:00.105668Z","shell.execute_reply.started":"2025-12-22T11:26:00.099567Z","shell.execute_reply":"2025-12-22T11:26:00.104911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ask it to do a calculation\nresponse = code_model.generate_content(\n    \"Calculate the first 10 Fibonacci numbers using Python\"\n)\n\nprint(\"=== CODE EXECUTION TEST ===\")\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T11:26:03.484856Z","iopub.execute_input":"2025-12-22T11:26:03.485534Z","iopub.status.idle":"2025-12-22T11:26:05.512072Z","shell.execute_reply.started":"2025-12-22T11:26:03.485501Z","shell.execute_reply":"2025-12-22T11:26:05.511288Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use the article from Phase 1\nsample_article = \"\"\"\nArtificial intelligence has transformed dramatically in recent years. \nMachine learning models have become increasingly sophisticated, enabling \napplications ranging from natural language processing to computer vision. \nCompanies across industries are adopting AI to improve efficiency and \ncreate new products. However, concerns about AI safety, bias, and \nethical implications continue to be debated. Researchers are working \non making AI systems more transparent and accountable. The future of \nAI development will likely focus on creating systems that are both \npowerful and aligned with human values. Education and workforce \ntraining are also adapting to prepare people for an AI-driven economy.\n\"\"\"\n\n# Ask Gemini to analyze it with code\nanalysis_prompt = f\"\"\"\nAnalyze this text using Python code:\n\nText: {sample_article}\n\nWrite Python code to calculate:\n1. Total word count\n2. Total sentence count  \n3. Average words per sentence\n4. Top 5 most common words\n5. Longest sentence\n\nRun the code and show the results.\n\"\"\"\n\nresponse = code_model.generate_content(analysis_prompt)\n\nprint(\"=== TEXT ANALYSIS (Capability 3) ===\")\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T11:26:08.644422Z","iopub.execute_input":"2025-12-22T11:26:08.644784Z","iopub.status.idle":"2025-12-22T11:26:19.536332Z","shell.execute_reply.started":"2025-12-22T11:26:08.644755Z","shell.execute_reply":"2025-12-22T11:26:19.535538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_text_with_code(text, code_model):\n    \"\"\"\n    Capability 3: Code Execution Analysis\n    Uses AI to write and run Python code for text analysis\n    \n    Args:\n        text: The text to analyze\n        code_model: Gemini model with code_execution enabled\n        \n    Returns:\n        str: Analysis results with code output\n    \"\"\"\n    \n    prompt = f\"\"\"\n    Analyze this text using Python:\n    \n    Text: {text}\n    \n    Write Python code to calculate:\n    1. Total word count\n    2. Number of unique words\n    3. Average sentence length\n    4. Longest word\n    5. Any interesting patterns\n    \n    Execute the code and report the results clearly.\n    \"\"\"\n    \n    response = code_model.generate_content(prompt)\n    return response.text\n\n# Test it!\nprint(\"=== DETAILED ANALYSIS ===\")\nanalysis = analyze_text_with_code(sample_article, code_model)\nprint(analysis)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T11:26:27.198831Z","iopub.execute_input":"2025-12-22T11:26:27.199626Z","iopub.status.idle":"2025-12-22T11:26:33.993908Z","shell.execute_reply.started":"2025-12-22T11:26:27.199594Z","shell.execute_reply":"2025-12-22T11:26:33.993080Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Combine everything\n\nimport google.generativeai as genai\n\n# Paste your FRESH key here\nAPI_KEY = 'AIzaSyAdyo5cEPVjzysw2IsRIcVTEtCDhICyHio'\n\nmodel = genai.GenerativeModel('models/gemini-2.0-flash')\ncode_model = genai.GenerativeModel('models/gemini-2.0-flash', tools='code_execution')\n\ndef full_document_analysis(text, model, code_model):\n    \"\"\"\n    Complete analysis using ALL 3 Gen AI capabilities:\n    1. Summarization\n    2. Similarity search\n    3. Code execution\n    \"\"\"\n    \n    print(\"=\" * 70)\n    print(\"SMART DOCUMENT ANALYZER - Complete Analysis\")\n    print(\"=\" * 70)\n    \n    # CAPABILITY 1: Summarization\n    print(\"\\nðŸ“ CAPABILITY 1: SUMMARIZATION\")\n    print(\"-\" * 70)\n    summary_prompt = f\"Summarize this in 2-3 sentences:\\n\\n{text}\"\n    summary = model.generate_content(summary_prompt)\n    print(summary.text)\n    \n    # CAPABILITY 2: Key Sentences (Embeddings)\n    print(\"\\nðŸ” CAPABILITY 2: KEY SENTENCES (Embeddings)\")\n    print(\"-\" * 70)\n    sentences = [s.strip() + '.' for s in text.split('.') if s.strip()]\n    if len(sentences) > 1:\n        query = \"What is the main point?\"\n        results = find_most_similar(query, sentences)\n        print(\"Most relevant sentences:\")\n        for i, result in enumerate(results[:3], 1):\n            print(f\"{i}. [{result['similarity']:.3f}] {result['text']}\")\n    \n    # CAPABILITY 3: Statistical Analysis (Code Execution)\n    print(\"\\nðŸ“Š CAPABILITY 3: STATISTICAL ANALYSIS (Code Execution)\")\n    print(\"-\" * 70)\n    analysis = analyze_text_with_code(text, code_model)\n    print(analysis)\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(\"âœ… Complete Analysis Done!\")\n    print(\"=\" * 70)\n\n\n# Run the complete analysis!\nfull_document_analysis(sample_article, model, code_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T11:27:14.880790Z","iopub.execute_input":"2025-12-22T11:27:14.881641Z","iopub.status.idle":"2025-12-22T11:27:14.977418Z","shell.execute_reply.started":"2025-12-22T11:27:14.881608Z","shell.execute_reply":"2025-12-22T11:27:14.976267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Check out models that can do 'generateConent'\n\nimport google.generativeai as genai\n\nprint(\"=== YOUR AVAILABLE MODELS ===\\n\")\n\navailable_models = []\n\nfor m in genai.list_models():\n    if 'generateContent' in m.supported_generation_methods:\n        print(f\"âœ… {m.name}\")\n        available_models.append(m.name)\n\nprint(f\"\\n Found {len(available_models)} models you can use!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T22:53:05.477177Z","iopub.execute_input":"2025-12-21T22:53:05.477608Z","iopub.status.idle":"2025-12-21T22:53:05.836209Z","shell.execute_reply.started":"2025-12-21T22:53:05.477575Z","shell.execute_reply":"2025-12-21T22:53:05.834850Z"}},"outputs":[],"execution_count":null}]}